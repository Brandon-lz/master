{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#设置log信息等级，2代表只打印重要信息\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 211s 18us/step\n"
     ]
    }
   ],
   "source": [
    "#加载mnist数据集，如果本地没有数据缓存，则会从网络中下载数据\n",
    "# x: [60k,28,28]\n",
    "# y: [60k]\n",
    "(x,y),_ = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据集转化为tensor格式\n",
    "x = tf.convert_to_tensor(x,dtype=tf.float32)/255.      #/255.是将数据转化为0-1.之间\n",
    "y = tf.convert_to_tensor(y,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([60000, 28, 28]), TensorShape([60000]), tf.float32, tf.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看x数据集中的最大值与最小值\n",
    "tf.reduce_max(x),tf.reduce_min(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=int32, numpy=9>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(y),tf.reduce_min(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)       #将数据集按128个单位长度进行分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 28, 28]), TensorShape([128]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_db)     #转化为生成器\n",
    "sample = next(train_iter)\n",
    "sample[0].shape,sample[1].shape     #这里可以看到batch的作用是吧x,y数据集按128一个单位进行分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[b,784]=> [b,256] => [b,128] =>[b,10]\n",
    "#那么，w和b的矩阵大小需要满足矩阵相乘的规律\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784,256]))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256,128]))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128,10]))\n",
    "b3 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-506bf8606c5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m#w1 = w1 - lr * w1_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "#进行迭代\n",
    "for (x,y) in train_db:\n",
    "    #x：[128,28,28]\n",
    "    #y:[128]\n",
    "    x = tf.reshape(x,[-1,28*28])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        #x:[b,28*28]\n",
    "        #h1 = x@w1 + b1\n",
    "        #[b,784]@[784,256] + [256] = [b,256]\n",
    "        h1 = x@w1 + b1\n",
    "        h1 = tf.nn.relu(h1)   #非线性函数，去掉负数\n",
    "\n",
    "        #[b,256]@[256,128]+[128] = [b,128]\n",
    "        h2 = h1@w2 + b2\n",
    "        h2 = tf.nn.relu(h2)\n",
    "\n",
    "        #[b,128]@[128,10] = [b,10]\n",
    "        out = h2@w3 + b3\n",
    "\n",
    "\n",
    "        #compute loss\n",
    "        y_onehot = tf.one_hot(y,depth=10)     #编码\n",
    "\n",
    "        #mse = mean((y-out)^2)    计算方差\n",
    "        loss = tf.square(y_onehot - out)\n",
    "\n",
    "        #mean:scalar\n",
    "        loss = tf.reduce_mean(loss)\n",
    "    grads = tape.gradient(loss,[w1,b1,w2,b2,w3,b3])\n",
    "    #w1 = w1 - lr * w1_grad\n",
    "    w1 = w1-lr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
